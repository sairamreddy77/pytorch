{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLE NEURAL NETWORK IN PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn**\n",
    "\n",
    "Neural networks can be constructed using the torch.nn package.<br>\n",
    "\n",
    "Provides pretty much all neural network related functionalities such as :<br>\n",
    "\n",
    "Linear layers - nn.Linear, nn.Bilinear <br>\n",
    "Convolution Layers - nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d<br>\n",
    "Nonlinearities - nn.Sigmoid, nn.Tanh, nn.ReLU, nn.LeakyReLU<br>\n",
    "Pooling Layers - nn.MaxPool1d, nn.AveragePool2d<br>\n",
    "Recurrent Networks - nn.LSTM, nn.GRU<br>\n",
    "Normalization - nn.BatchNorm2d<br>\n",
    "Dropout - nn.Dropout, nn.Dropout2d<br>\n",
    "Embedding - nn.Embedding<br>\n",
    "Loss Functions - nn.MSELoss, nn.CrossEntropyLoss, nn.NLLLoss<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self , input_size, num_classes):   #28X28 pixel size   and total 10 classes\n",
    "        super(NN,self).__init__()   \n",
    "        self.fc1= nn.Linear(input_size, 50)\n",
    "        self.fc2=nn.Linear(50, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self , x):\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)    #activation function\n",
    "        x= self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "model=NN(784, 10)           #784 = 28X28 pixel (input size)  and 10 is the number of classes (output size)\n",
    "x =torch.randn(64,784)      #we are creating a 64x784 which means 64 examples of 784 pixels (64 random training examples)\n",
    "model(x).shape              #predicting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "num_classes=10\n",
    "learning_rate=0.001\n",
    "batch_size=64\n",
    "num_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#totransforms will convert the PIL image or the ndarray to tensors\n",
    "#train_dataset will download the dataset\n",
    "#train_loader and test_loader will load the dataset\n",
    "#dataloader will load the dataset in batches of batchsize\n",
    "# shuffle=True will jumble all the images in the dataset so that the model is not biased towards any particular image\n",
    "#shuffle =True will make sure that the same images are present in a a batch in a different epoch\n",
    "#root='dataset/' is the where the images to be stored\n",
    "\n",
    "train_dataset= datasets.MNIST(root='dataset/', train=True , transform=transforms.ToTensor() , download=True)\n",
    "train_loader=DataLoader(dataset=train_dataset , batch_size=batch_size , shuffle= True)\n",
    "test_dataset=datasets.MNIST(root='dataset/', train=False , transform=transforms.ToTensor() , download=True)\n",
    "test_loader=DataLoader(dataset=test_dataset , batch_size=batch_size , shuffle= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize loss and optim function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "model=NN(input_size=input_size , num_classes=num_classes).to(device)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss() #loss function (log loss) multi class\n",
    "optimizer=optim.Adam(model.parameters() , lr=learning_rate)      #requires model parameters \n",
    "#Adam is faster to converge. SGD is slower but generalizes better. use accor to situation.\n",
    "\n",
    "#learning rate is the rate at which the model learns from the data\n",
    "#If it is too high it means that the model will learn too fast and will not be able to find the optimal solution\n",
    "#If it is too low, the model will learn too slow and will not be able to find the optimal solution.  so find balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist dataset has 939X64 training examples(60k)\n",
    "#batch_idx is the batch num\n",
    "#64X1X28X28     28X28 image size(pixels)  , 1 is for grayscale image (no rgb)  , 64 is batch size\n",
    "#64X1X28X28 we convert this to 64 X 784 (1X28X28)  we flattened 1X28X28 into single dim and we have 64 training ex\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data=data.to(device=device)\n",
    "        targets=targets.to(device=device)\n",
    "        \n",
    "        #reshape\n",
    "        data=data.reshape(data.shape[0],-1)\n",
    "\n",
    "        #forward\n",
    "        outputs=model(data)                #produces outputs , out.shape = [64,10] , for each training ex give 10 values(probs of 10 classes)\n",
    "        loss=criterion(outputs,targets)    #loss func , find loss btw models predictions (outputs) and true labels(targets)\n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()              #Before backpropagation, you need to zero out (reset) the gradients of all the parameters.\n",
    "        loss.backward()                    #the gradients of the loss are computed\n",
    "        \n",
    "        #gradient descent or adam step\n",
    "        optimizer.step()                   #the optimizer adjusts the weights and biases (i.e., the parameters) based on computed gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks the accuarcy of a model on a given dataset\n",
    "\n",
    "#model.eval - This line puts the model in evaluation mode, which turns off certain behaviors specific to training, such as dropout and \n",
    "#batch normalization updates. In evaluation mode, the model performs inference without changing internal states like in training.\n",
    "\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('checking accuracy on training data')\n",
    "    else:\n",
    "        print('checking accuracy on test data')\n",
    "\n",
    "    num_correct=0\n",
    "    num_samples=0 \n",
    "    model.eval()   #evaluation mode\n",
    "  \n",
    "    with torch.no_grad():           #torch.no_grad() is a context manager that disables gradient calculation\n",
    "        for x, y in loader: \n",
    "            x=x.to(device=device)\n",
    "            y=y.to(device=device)\n",
    "            x=x.reshape(x.shape[0],-1)\n",
    "\n",
    "            scores=model(x)         #outputs [64,10]\n",
    "\n",
    "            _, predictions = scores.max(dim=1)     \n",
    "            #max at each row (so total 64 rows and 10 cols and we choose 1 value at each row )\n",
    "            # '_' (first returned val) returns the values (max values of logits or probabilities) \n",
    "            # 'predictions' (Second returned val) returns the indices(max val indices) \n",
    "                                                \n",
    "            num_correct += (predictions==y).sum()    #.sum() counts how many predictions are correct in the current batch.\n",
    "            num_samples += predictions.size(0)       #keeps track of the total number of samples processed.\n",
    "\n",
    "\n",
    "        print(f'got {num_correct} / {num_samples} with acuuracy {float(num_correct) / float(num_samples) * 100:.2f}')\n",
    "\n",
    "    model.train()   #sets the model back to training mode from eval mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking accuracy on training data\n",
      "got 58318 / 60000 with acuuracy 97.20\n",
      "checking accuracy on test data\n",
      "got 9663 / 10000 with acuuracy 96.63\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader,model)\n",
    "check_accuracy(test_loader,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
